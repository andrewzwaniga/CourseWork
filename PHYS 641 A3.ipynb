{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS 641 Assignment 3\n",
    "### Andrew V. Zwaniga\n",
    "### (260843983) \n",
    "\n",
    "\n",
    "\n",
    "# Note: \n",
    "\n",
    "__I unzipped `LOSC_Event_tutorial.zip` in the directory where my code was made so this code should run as long as it is placed in the same directory as where you unzipped your copy of the data.__ \n",
    "\n",
    "\n",
    "\n",
    "## Problem 1: Finding gravitational waves in LIGO data \n",
    "\n",
    "We will investigate a data set obtained from the file `LOSC_Event_tutorial` that is publicly available at https://www.gw-openscience.org/data/. Contained is data from the Laser Interferometer Gravitational wave Observatory (LIGO) experiment that seeks to detect gravitational waves at Earth from exotic binary mergers like BH-BH binaries, BH-NS binaries, and NS-NS binaries that merge. \n",
    "\n",
    "The plan is to look at data from different events from the Hanford (H) and Livingston (L) observatories separately and come up with a unique noise model for each. Then we will loop over the four events for each observatory and search for the gravitational wave. The steps for working with the data will be as follows: \n",
    "\n",
    "### Part A: Discussion of the steps taken to estimate the noise matrix\n",
    "\n",
    "#### (1) \n",
    "Load the raw data and apply a suitably chosen __window function__ on the samples. \n",
    "\n",
    " - A __window function__ is one that goes to zero smoothly at either boundary of a finite interval. The purpose of this is to prevent effects of the periodicity of discrete Fourier transforms. By multiplying the window function by the data, the new dataset will go to zero on its boundary. \n",
    "\n",
    "\n",
    "#### (2) \n",
    "Compute the __power spectrum__ of the windowed data by taking the discrete Fourier transform of it. \n",
    "\n",
    " - If $f(x)$ is a discrete function on datapoints $x=1,\\dots,N$ and $F(k)$ is the discrete Fourier transform of $f(x)$ then the __power spectrum__ is $P(f(x)) = \\langle \\overline{F(k)}~F(k)\\rangle$. \n",
    " - The power spectrum provides an estimate of the noise matrix. \n",
    "\n",
    "#### (3) \n",
    "Smooth the power spectrum to manage spectral artifacts that present themselves as sharp spikes in the data. \n",
    "\n",
    " - Smoothing is performed by convolving with a __smooth function__. In the most general case, a function $f:\\mathbb{R}\\to\\mathbb{R}$ is said to be smooth at a point $x$ in its domain if it is smooth in an open neighbourhood $U$ of $x$. This in turn requires that on $U$, $\\frac{d^{n}f}{dx^{n}}$ exists for all $n \\in \\mathbb{N}$. \n",
    " - In this code, we smooth by using a Gaussian template function $g(x) = \\exp\\Big{(}-\\frac{x^{2}}{2\\sigma^{2}}\\Big{)}$ for $x \\in [-1,1]$. To actually carry out the smoothing, we effectively work with $\\mathfrak{g}(n) = g(n),~n\\in[0,\\tfrac{N}{2}]$ and $\\mathfrak{g}(n) = g(n-N),~n\\in[\\tfrac{N}{2}+1, N-1]$ for non-negative integers $n$. \n",
    "\n",
    "#### (4) \n",
    "__Whiten__ both the strain data and the template (\"chirp\") function that will be fitted. This is done by calculating the estimate of the inverse noise matrix, $({\\langle \\overline{F(k)}~F(k)\\rangle}_{\\text{smoothed}})^{-1}$. Then we calculate $N^{-\\tfrac{1}{2}} = \\sqrt{N^{-1}}$ and find $N^{-\\tfrac{1}{2}}\\mathbf{d}$ as well as $N^{-\\tfrac{1}{2}}A$.\n",
    "\n",
    "#### (5) \n",
    "Fit the whitened template (\"chirp\") function to the whitened strain data and find the linear least-squares best fit to the data. \n",
    "\n",
    " - Recall that for a template $A$ with parameters $\\mathbf{m}$ and a data set $\\mathbf{d}$ such that $\\langle\\mathbf{d}\\rangle = A\\mathbf{m}$, the choice of $\\mathbf{m}$ that minimizes $\\chi^{2}$ is $\\mathbf{m} = (A^{T}N^{-1}A)^{-1}A^{T}N^{-1}\\mathbf{d}$.\n",
    " \n",
    "### Part B: Using Python to implement the noise matrix estimate and fit the data for a gravitational wave\n",
    "\n",
    "Below is a script that attempts to realize the blueprint prescribed in __Part A__. \n",
    "\n",
    "#### Reading in the LIGO data \n",
    "\n",
    "I make use of the script made for our class `simple_read_ligo.py` to handle the reading of the h5py files. My code reads the `BBH_events_v3.json` file to get the correct event name, file names for the Hanford and Livingston data, and the separate templates for Hanford and Livingston. \n",
    "\n",
    "#### Helper functions \n",
    "\n",
    "There are two helper functions that I based on those we saw developed in class. I call them `smoothing_interval` and `make_smooth_gauss`. \n",
    "\n",
    "`smoothing_interval` redistributes the range of a function to produce a smoothing function on $[0,N-1] for $N$ data points. \n",
    "\n",
    "`make_smooth_gauss` smooths a data set by convolving with a Gaussian that has been modified by `smoothing_interval`. \n",
    "\n",
    "#### Windowing \n",
    "\n",
    "I experiment with three different windows in my code. \n",
    "\n",
    "(1) $w_{1}(x) = (1-x^{2})\\exp\\Big{(}-\\frac{x^{2}}{2\\sigma^{2}}\\Big{)}$\n",
    "\n",
    "(2) Nutall window, from https://en.wikipedia.org/wiki/Window_function. $w_2(x) = a_{0}- a_{1}\\cos(\\frac{2\\pi x}{N-1}) + a_{2}\\cos(\\frac{4\\pi x}{N-1}) - a_{3}\\cos(\\frac{6\\pi x}{N-1})$. \n",
    "\n",
    "$a_{0} = 0.355768, ~a_{1} = 0.487396, ~a_{2} = 0.144232, ~a_{3} = 0.012604$. \n",
    "\n",
    "(3) Blackman-Nutall window, from https://en.wikipedia.org/wiki/Window_function. $w_{3}(x) = a_{0}- a_{1}\\cos(\\frac{2\\pi x}{N-1}) + a_{2}\\cos(\\frac{4\\pi x}{N-1}) - a_{3}\\cos(\\frac{6\\pi x}{N-1})$. \n",
    "\n",
    "$a_{0} = 0.3635819, ~a_{1} = 0.4891775, ~a_{2} = 0.1365995, ~a_{3} = 0.0106411$.\n",
    "\n",
    "The choice of (1) is rather arbitrary - it is easy to think of a function that vanishes at $x = -1$ and $x = 1$ smoothly without the derivatives also vanishing there and I thought it would be interesting to test the utility of this function as a window. It performed poorly for these data sets. \n",
    "\n",
    "The choice of (2) and (3) is based on their Fourier transforms having suppressed side lobes. After looking at the data, this seemed like an attractive feature when considering that the noise is large on the boundaries of frequency space. \n",
    "\n",
    "#### Smoothing \n",
    "\n",
    "I chose to use a Gaussian template function for smoothing. Among its surprising properties, the function $g(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\Big{[}-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\Big{]}$ happens to belong to the class of Schwartz functions $S(\\mathbb{R})$. (This class is invariant under the Fourier transform.) When convolved with other functions it yields particularly nice results and it is easy to work with analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "\n",
    "def smoothing_interval(n):\n",
    "    # Takes an integer n \n",
    "    # Creates a vector of numbers 1,2,...,n \n",
    "    # Forces the last n/2 entries to be equal to the first n/2 subtract n \n",
    "    \n",
    "    x     = np.arange(n)\n",
    "    i     = int(n/2) \n",
    "    x[i:] = x[i:] - n \n",
    "    \n",
    "    assert(x[-1] == -1) # force the final entry in the vector to be -1, just in case it isn't \n",
    "    \n",
    "    return 1.0*x # need it to be a float     \n",
    "        \n",
    "def make_smooth_gauss(data, fwhm):\n",
    "    \n",
    "    # This function will convolve `data` with a Gaussian by first computing \n",
    "    # the FT of each and then taking their product; finally, taking the inverse FT \n",
    "    # will map back to the initial domain and preserve normalizations (by Parseval's theorem)\n",
    "    \n",
    "    n     = len(data)\n",
    "    x     = smoothing_interval(n)\n",
    "    sigma = fwhm/(2*np.sqrt(2*np.log(2))) # formula for finding sigma from fwhm  \n",
    "    \n",
    "    smoothie = np.exp(-(x**2)/(2*(sigma**2))) \n",
    "    smoothie = smoothie/(smoothie.sum())\n",
    "    \n",
    "    ft_data     = np.fft.rfft(data)\n",
    "    ft_smoothie = np.fft.rfft(smoothie)\n",
    "    \n",
    "    ft_product  = ft_data*ft_smoothie \n",
    "    \n",
    "    return np.fft.irfft(ft_product, n=n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event #1:\n",
      "------------------\n",
      "Reading file H-H1_LOSC_4_V2-1126259446-32.hdf5 ...\n",
      "Reading file L-L1_LOSC_4_V2-1126259446-32.hdf5 ...\n",
      "Template: GW150914_4_template.hdf5\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "************\n",
      "Calculations\n",
      "************\n",
      "\n",
      "SNR for H:            12.054440674239627\n",
      "SNR for L:            9.92573069171091\n",
      "SNR for combined H+L: 12.514352426773288\n",
      "Half-sum index for H is     3415\n",
      "Half-sum frequency for H is 106.71875 Hz\n",
      "Half-sum index for L is     3663\n",
      "Half-sum frequency for L is 114.46875 Hz\n",
      "--------//--------\n",
      "\n",
      "\n",
      "\n",
      "Processing event #2:\n",
      "------------------\n",
      "Reading file H-H1_LOSC_4_V2-1128678884-32.hdf5 ...\n",
      "Reading file L-L1_LOSC_4_V2-1128678884-32.hdf5 ...\n",
      "Template: LVT151012_4_template.hdf5\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "************\n",
      "Calculations\n",
      "************\n",
      "\n",
      "SNR for H:            5.918617397297965\n",
      "SNR for L:            5.2062965269458195\n",
      "SNR for combined H+L: 8.771821728009286\n",
      "Half-sum index for H is     2745\n",
      "Half-sum frequency for H is 85.78125 Hz\n",
      "Half-sum index for L is     3171\n",
      "Half-sum frequency for L is 99.09375 Hz\n",
      "--------//--------\n",
      "\n",
      "\n",
      "\n",
      "Processing event #3:\n",
      "------------------\n",
      "Reading file H-H1_LOSC_4_V2-1135136334-32.hdf5 ...\n",
      "Reading file L-L1_LOSC_4_V2-1135136334-32.hdf5 ...\n",
      "Template: GW151226_4_template.hdf5\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "************\n",
      "Calculations\n",
      "************\n",
      "\n",
      "SNR for H:            8.568934556365592\n",
      "SNR for L:            4.16851600500279\n",
      "SNR for combined H+L: 12.931063793028253\n",
      "Half-sum index for H is     2778\n",
      "Half-sum frequency for H is 86.8125 Hz\n",
      "Half-sum index for L is     3520\n",
      "Half-sum frequency for L is 110.0 Hz\n",
      "--------//--------\n",
      "\n",
      "\n",
      "\n",
      "Processing event #4:\n",
      "------------------\n",
      "Reading file H-H1_LOSC_4_V1-1167559920-32.hdf5 ...\n",
      "Reading file L-L1_LOSC_4_V1-1167559920-32.hdf5 ...\n",
      "Template: GW170104_4_template.hdf5\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "Saving Hanford plot...\n",
      "Saving Livingston plot...\n",
      "\n",
      "************\n",
      "Calculations\n",
      "************\n",
      "\n",
      "SNR for H:            7.707267057520062\n",
      "SNR for L:            6.06895462737636\n",
      "SNR for combined H+L: 10.403074901962743\n",
      "Half-sum index for H is     3000\n",
      "Half-sum frequency for H is 93.75 Hz\n",
      "Half-sum index for L is     2487\n",
      "Half-sum frequency for L is 77.71875 Hz\n",
      "--------//--------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import simple_read_ligo as rl\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import pylab\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from pylab import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "#------------------------------------------------------# \n",
    "# Load the data from the .json file \n",
    "#------------------------------------------------------#\n",
    "\n",
    "nevents = 4\n",
    "\n",
    "with open(\"BBH_events_v3.json\") as f:\n",
    "    events = json.load(f)\n",
    "    \n",
    "event_names = [\"GW150914\", \"LVT151012\", \"GW151226\", \"GW170104\"]\n",
    "\n",
    "#------------------------------------------------------# \n",
    "# Loop over the events  \n",
    "#------------------------------------------------------#\n",
    "\n",
    "for i in range(nevents):\n",
    "    \n",
    "    print(\"Processing event #\" + str(i+1) + \":\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Get the event dict from the .json file \n",
    "    event_dict = events[event_names[i]]\n",
    "    \n",
    "    fn_H1 = event_dict[\"fn_H1\"]    # Hanford file name\n",
    "    fn_L1 = event_dict[\"fn_L1\"]    # Livingston file name\n",
    "    \n",
    "    # Read the files \n",
    "    print(\"Reading file \" + fn_H1 + \" ...\")\n",
    "    strain_H, dt_H, utc_H = rl.read_file(fn_H1)\n",
    "    print(\"Reading file \" + fn_L1 + \" ...\")\n",
    "    strain_L, dt_L, utc_L = rl.read_file(fn_L1)\n",
    "    \n",
    "    s_H = len(strain_H)\n",
    "    s_L = len(strain_L)\n",
    "    #print(\"s_H == s_L: \" + str(np.allclose(s_H, s_L))) # -> TRUE\n",
    "    #print(\"t_H == t_L: \" + str(np.allclose(t_H, t_L))) # -> TRUE \n",
    "    \n",
    "    n  = s_H\n",
    "    dt = dt_H\n",
    "    t  = np.arange(n)*dt \n",
    "    \n",
    "    label_H = \"H\" + str(i+1) + \"_\" + event_names[i]\n",
    "    label_L = \"L\" + str(i+1) + \"_\" + event_names[i]\n",
    "    \n",
    "    \"\"\" Plotting\n",
    "        --------   \n",
    "    \n",
    "    plt.loglog(t, strain_H, linewidth=2, label=label_H)\n",
    "    plt.loglog(t, strain_L, linewidth=2, label=label_L)\n",
    "    if i==3: \n",
    "        plt.title(\"Advanced LIGO: measured strain for 4 events from Hanford (H) and Livingston (L)\")\n",
    "        plt.xlabel(\"Log$_{10}$(Time since event [s])\")\n",
    "        plt.ylabel(\"Log$_{10}$(Strain)\")\n",
    "        legend(loc=\"lower left\")\n",
    "        name = \"plot_all_strains.png\"\n",
    "        pylab.savefig(name)\n",
    "        print(\"Saved figure \" + name)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get the template functions \n",
    "    fn_template = event_dict[\"fn_template\"]\n",
    "    print(\"Template: \" + fn_template)\n",
    "    temp_H, temp_L = rl.read_template(fn_template)     \n",
    "    \n",
    "    # Try some window functions \n",
    "    \n",
    "    #x = np.linspace(-1, 1, n)\n",
    "    x = np.arange(n)\n",
    "\n",
    "    #---------------#\n",
    "    # window option 1\n",
    "    #---------------#\n",
    "    \n",
    "    # Reference: a function of my own creation; it goes to a very small value at -1, 1; the \n",
    "    # 1st derivative goes to a larger nonzero value but both can be adjusted by sigma. \n",
    "    \n",
    "    sigma = 0.3 # window parameter \n",
    "    w1 = (-x**2+1)*np.exp(-x**2/(2*sigma**2)) \n",
    "    \n",
    "    # N.B. Post-processing note: too much unwhitened noise is leftover on the ends of the time \n",
    "    # interval, the signal to noise ratio would be low! \n",
    "    \n",
    "    # --------------# \n",
    "    # window option 2 \n",
    "    # --------------#\n",
    "    \n",
    "    # Reference: https://en.wikipedia.org/wiki/Window_function \"Nutall window\"\n",
    "    \n",
    "    a0 = 0.355768 \n",
    "    a1 = 0.487396\n",
    "    a2 = 0.144232\n",
    "    a3 = 0.012604\n",
    "    pi = math.pi \n",
    "    \n",
    "    w2 = a0 - a1*np.cos((2*pi*x)/(n-1)) + a2*np.cos((4*pi*x)/(n-1)) - a3*np.cos((6*pi*x)/(n-1))\n",
    "    \n",
    "    # N.B. Post-processing note: large artifacts leftover at the lower bound of the time interval \n",
    "    \n",
    "    #---------------#\n",
    "    # window option 3\n",
    "    #---------------#\n",
    "    \n",
    "    # Reference: https://en.wikipedia.org/wiki/Window_function \"Blackman-Nutall window\"\n",
    "    \n",
    "    a0 = 0.3635819 \n",
    "    a1 = 0.4891775\n",
    "    a2 = 0.1365995\n",
    "    a3 = 0.0106411\n",
    "    \n",
    "    w3 = a0 - a1*np.cos((2*pi*x)/(n-1)) + a2*np.cos((4*pi*x)/(n-1)) - a3*np.cos((6*pi*x)/(n-1))\n",
    "    \n",
    "    # N.B. Post-processing note: this window seems to work the best; there are some strange \n",
    "    # artifacts at the ends of the intervals similar to what w2 did, but they are much smaller \n",
    "    \n",
    "    #---------------#\n",
    "    \n",
    "    # Choose a window now \n",
    "    #w = w1 \n",
    "    #w = w2\n",
    "    w = w3\n",
    "    \n",
    "    #plt.plot(w)\n",
    "    #plt.show()\n",
    "    \n",
    "    ft_strain_H = np.fft.rfft(strain_H*w)\n",
    "    ft_strain_L = np.fft.rfft(strain_L*w)\n",
    "    ps_H        = (np.abs(ft_strain_H))**2  # absolute value, just as an artifact from complex case \n",
    "    ps_L        = (np.abs(ft_strain_L))**2 \n",
    "    \n",
    "    dnu = 1/(n*dt)\n",
    "    nu  = np.arange(len(ps_H))*dnu\n",
    "    \n",
    "    \"\"\" Plotting\n",
    "        --------\n",
    "    \n",
    "    plt.loglog(nu, ps_H, linewidth=2, label=label_H)\n",
    "    plt.loglog(nu, ps_L, linewidth=2, label=label_L)\n",
    "    if i==3: \n",
    "        plt.title(\"Advanced LIGO: power spectrum for 4 events from Hanford (H) and Livingston (L)\")\n",
    "        plt.xlabel(\"Log$_{10}$(Frequency [Hz])\")\n",
    "        plt.ylabel(\"Log$_{10}$(Power)\")\n",
    "        legend(loc=\"lower left\")\n",
    "        name = \"plot_all_ps.png\"\n",
    "        pylab.savefig(name)\n",
    "        print(\"<-! Saved figure \" + name)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Now smooth the power spectra \n",
    "\n",
    "    smooth_ps_H = make_smooth_gauss(ps_H, 30)\n",
    "    smooth_ps_L = make_smooth_gauss(ps_L, 30)\n",
    "    \n",
    "    #plt.loglog(nu, smooth_ps_H)\n",
    "    #plt.loglog(nu, smooth_ps_L)\n",
    "    \n",
    "    \"\"\" Plotting\n",
    "        --------\n",
    "        \n",
    "    plt.loglog(nu, smooth_ps_H, linewidth=2, label=label_H)\n",
    "    plt.loglog(nu, smooth_ps_L, linewidth=2, label=label_L)\n",
    "    if i==3: \n",
    "        plt.title(\"Advanced LIGO: smoothed power spectrum for 4 events from Hanford (H) and Livingston (L)\")\n",
    "        plt.xlabel(\"Log$_{10}$(Frequency [Hz])\")\n",
    "        plt.ylabel(\"Log$_{10}$(Power)\")\n",
    "        legend(loc=\"lower left\")\n",
    "        name = \"plot_all_smoothed_ps.png\"\n",
    "        pylab.savefig(name)\n",
    "        print(\"<-! Saved figure \" + name)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now try whitening the smoothed PS by calculating N^{-0.5}*data \n",
    "    # With stationary noise, this is easily done: \n",
    "    \n",
    "    Ni_H = 1.0/smooth_ps_H   # inverse of (estimated!) noise matrix \n",
    "    Ni_L = 1.0/smooth_ps_L \n",
    "    \n",
    "    # Before we apply the inverse square root to the data, we need to be careful. \n",
    "    # Some parts of the smoothed power spectrum have very sharp spectral lines. \n",
    "    # Let's remove a chunk at the lower frequency end and the higher frequency end. \n",
    "    \n",
    "    # Theses dissections were decided after looking at the data to see what spectral \n",
    "    # features there are at low and high frequencies. It appears that ~< 10 Hz is \n",
    "    # very noisy and that >~ 1400 Hz has a bizarre structure that is assumed to be \n",
    "    # artificial. (The explanation of the structure is beyond the scope of this work.)\n",
    "    \n",
    "    Ni_H[nu <= 10]   = 0\n",
    "    Ni_H[nu >= 1300] = 0\n",
    "    \n",
    "    Ni_L[nu <= 10]   = 0\n",
    "    Ni_L[nu >= 1300] = 0\n",
    "    \n",
    "    whitened_ft_strain_H = np.sqrt(Ni_H)*ft_strain_H\n",
    "    whitened_ft_strain_L = np.sqrt(Ni_L)*ft_strain_L\n",
    "    \n",
    "    whitened_strain_H = np.fft.irfft(whitened_ft_strain_H)\n",
    "    whitened_strain_L = np.fft.irfft(whitened_ft_strain_L)\n",
    "    \n",
    "    #plt.plot(t, whitened_strain_H)\n",
    "    #plt.plot(t, whitened_strain_L)\n",
    "    \n",
    "    \"\"\" Plotting\n",
    "        --------\n",
    "    \n",
    "    plt.plot(t, whitened_strain_H, linewidth=2, label=label_H)\n",
    "    plt.plot(t, whitened_strain_L, linewidth=2, label=label_L)\n",
    "    if i==3: \n",
    "        plt.title(\"Advanced LIGO: whitened strain for 4 events from Hanford (H) and Livingston (L)\")\n",
    "        plt.xlabel(\"Log$_{10}$(Time since event [s])\")\n",
    "        plt.ylabel(\"Log$_{10}$(Strain)\")\n",
    "        legend(loc=\"lower left\")\n",
    "        name = \"plot_all_whitened_strains.png\"\n",
    "        pylab.savefig(name)\n",
    "        print(\"<-! Saved figure \" + name)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now to fit the corresponding template for each event \n",
    "    # First get the FT of the windowed template, then whiten as above\n",
    "    \n",
    "    ft_temp_H          = np.fft.rfft(temp_H*w)\n",
    "    whitened_ft_temp_H = ft_temp_H*np.sqrt(Ni_H)\n",
    "    whitened_temp_H    = np.fft.irfft(whitened_ft_temp_H)\n",
    "    \n",
    "    ft_temp_L          = np.fft.rfft(temp_L*w)\n",
    "    whitened_ft_temp_L = ft_temp_L*np.sqrt(Ni_L)\n",
    "    whitened_temp_L    = np.fft.irfft(whitened_ft_temp_L)\n",
    "    \n",
    "    # Now we shift the whitened FT template along every point of the whitened FT strain \n",
    "    # and multiply them pointwise; since the Fourier transform of a convolution is the \n",
    "    # pointwise multiplication of the Fourier transforms then taking inverse Fourier \n",
    "    # transform of a pointwise multiplication yields a convolution \n",
    "    \n",
    "    m_H = np.fft.irfft(whitened_ft_strain_H*np.conj(whitened_ft_temp_H))\n",
    "    m_L = np.fft.irfft(whitened_ft_strain_L*np.conj(whitened_ft_temp_L))\n",
    "    \n",
    "    ft_m_H = np.real(np.fft.rfft(m_H))\n",
    "    ft_m_L = np.real(np.fft.rfft(m_L))\n",
    "    \n",
    "    plt.plot(t, m_H)\n",
    "    print(\"Saving Hanford plot...\")\n",
    "    pylab.savefig(\"plot_H\" + str(i+1) + \"_\" + event_names[i] + \".png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(t, m_L)\n",
    "    print(\"Saving Livingston plot...\\n\")\n",
    "    pylab.savefig(\"plot_L\" + str(i+1) + \"_\" + event_names[i] + \".png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(nu, ft_m_H)\n",
    "    print(\"Saving Hanford plot...\")\n",
    "    pylab.savefig(\"plot_ft_H\" + str(i+1) + \"_\" + event_names[i] + \".png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.plot(nu, ft_m_L)\n",
    "    print(\"Saving Livingston plot...\\n\")\n",
    "    pylab.savefig(\"plot_ft_L\" + str(i+1) + \"_\" + event_names[i] + \".png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    print(\"************\")\n",
    "    print(\"Calculations\")\n",
    "    print(\"************\\n\")\n",
    "    \n",
    "    amp_H = np.max(np.abs(m_H))\n",
    "    amp_L = np.max(np.abs(m_L))\n",
    "    \n",
    "    # We estimate the noise in the signal by taking the \n",
    "    # standard deviation of points in the neighbourhood [0:3000] of the signal spike\n",
    "    \n",
    "    err_amp_H = np.std(m_H[0:3000]) \n",
    "    err_amp_L = np.std(m_L[0:3000])\n",
    "    \n",
    "    SNR_H = amp_H/err_amp_H\n",
    "    SNR_L = amp_L/err_amp_L \n",
    "    \n",
    "    combo = np.abs(m_H) + np.abs(m_L)\n",
    "    \n",
    "    amp_combo     = np.max(np.abs(combo))\n",
    "    err_amp_combo = np.std(combo[0:3000])\n",
    "    SNR_combo     = amp_combo/err_amp_combo \n",
    "    \n",
    "        \n",
    "    print(\"SNR for H:            \" + str(SNR_H))\n",
    "    print(\"SNR for L:            \" + str(SNR_L))\n",
    "    print(\"SNR for combined H+L: \" + str(SNR_combo))\n",
    "    \n",
    "    # Now for the last part, we want to find the frequency such that half the weight \n",
    "    # comes from higher frequencies and the other half of the weight comes from lower \n",
    "    # frequencies \n",
    "    \n",
    "    mmT_H = (np.dot(ft_m_H, ft_m_H.transpose()))\n",
    "    mmT_L = (np.dot(ft_m_L, ft_m_L.transpose()))\n",
    "    \n",
    "    avg_mmT_H = np.average(mmT_H)\n",
    "    avg_mmT_L = np.average(mmT_L)\n",
    "    \n",
    "    #print(\"<mmT> for H: \" + str(avg_mmT_H))\n",
    "    #print(\"<mmT> for L: \" + str(avg_mmT_L))\n",
    "    \n",
    "    summand = 0 \n",
    "    index   = -1\n",
    "    j = 0\n",
    "    \n",
    "    #print(\"t.size  = \" + str(t.size))\n",
    "    #print(\"nu.size = \" + str(nu.size))\n",
    "    \n",
    "    while summand < 0.5*avg_mmT_H: \n",
    "        summand += ft_m_H[j]**2\n",
    "        index = j\n",
    "        j += 1\n",
    "    \n",
    "    print(\"Half-sum index for H is     \" + str(index))\n",
    "    print(\"Half-sum frequency for H is \" + str(nu[index]) + \" Hz\")\n",
    "    \n",
    "    summand = 0 \n",
    "    index   = -1\n",
    "    j = 0\n",
    "    \n",
    "    while summand < 0.5*avg_mmT_L: \n",
    "        summand += ft_m_L[j]**2\n",
    "        index = j\n",
    "        j += 1\n",
    "    \n",
    "    print(\"Half-sum index for L is     \" + str(index))\n",
    "    print(\"Half-sum frequency for L is \" + str(nu[index]) + \" Hz\")\n",
    "    \n",
    "    \"\"\"\n",
    "    t_test = np.abs(amp_H - amp_L)/np.sqrt(err_amp_H**2 + err_amp_L**2)\n",
    "    \n",
    "    amp_avg = -1\n",
    "    err_avg = -1\n",
    "    \n",
    "    if t_test <= 2:\n",
    "        print(\"Consistent results.\")\n",
    "        amp_avg = (err_amp_L**2*amp_H + err_amp_H**2*amp_L)/(err_amp_H**2 + err_amp_L**2)\n",
    "        err_avg = np.sqrt(err_amp_H**2 + err_amp_L**2)\n",
    "    else:\n",
    "        print(\"Inconsistent results.\")\n",
    "        amp_avg = (amp_H+amp_L)/2\n",
    "        err_avg = (np.abs(amp_H - amp_L) + err_amp_H + err_amp_L)/2\n",
    "        \n",
    "    print(\"Averaged amplitude: \" + str(amp_avg))\n",
    "    print(\"Averaged error:     \" + str(err_avg))\n",
    "    \n",
    "    SNR_combined = amp_avg/err_avg\n",
    "    \n",
    "    print(\"Combined SNR:       \" + str(SNR_combined))\n",
    "    \"\"\"\n",
    "    print (\"--------//--------\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
